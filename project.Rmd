```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev="httpgd", fig.show="hold")
```

```{r}
library(dplyr)
library(tseries)
library(forecast)
library(lubridate)
```

```{r}
gold <- read.csv("Gold Price.csv")
```

```{r}
gold$Date <- as.Date(gold$Date)
gold <- gold[order(gold$Date),]
```

```{r}
tail(gold)
```

Data preparation
```{r}
#gold price statistics
summary(gold$Price)
```

```{r}
# check if there is missing value
sum(is.na(gold))

# detect outliers using IQR
boxplot(gold$Price, main="Gold Price Boxplot", col="lightblue", ylab = "price")

Q1 <- quantile(gold$Price, 0.25)
Q3 <- quantile(gold$Price, 0.75)
IQR_value <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

outliers <- gold$Price[gold$Price < lower_bound | gold$Price > upper_bound]
print(outliers)

```

```{r}
plot(gold$Date, gold$Price, ylab = "price", xlab = "date")
```
From the plot of the gold price, the plot seems not stationary because the trend of the price is increasing.

Check stationary using Augmented Dickey-Fuller (ADF test):
```{r}
adf.test(gold$Price)
```
By the Augmented Dickey-Fuller Test, the p-value is greater then alpha=0.05, which means we don't have enough evidence to reject that the data is not stationary. 


```{r}
# check symmetry
hist(gold$Price, main = "Histogram of the Gold Price", xlab = "Gold Price")
```
The histogram is right skewed.Try log transformation.

```{r}
#log transformation
gold$logPrice <- log(gold$Price)

#check symmetry
hist(gold$logPrice, main = "Histogram of Log Gold Price", xlab = "Log of Gold Price")
```
```{r}
#adf test after log transformation
adf.test(gold$logPrice)
```
Log transformation does not perform well. Try BoxCox transformation.

```{r}
#BoxCox transformation
lambda = BoxCox.lambda(gold$Price)
print(paste("best lambda is:" ,lambda))
```

```{r}
gold$boxcox = BoxCox(gold$Price, lambda = lambda)
hist(gold$boxcox, main = "histogram of BoxCox transformation", xlab = "gold price after boxcox transformation")
```

```{r}
#adf test after boxcox transformation
adf.test(gold$boxcox)
```
BoxCox transformation dose not works well. However, p-value decreases from 0.9002 to 0.3328 and the right skewed trend is weakened. We choose to retain BoxCox transformation.

```{r}
#mean trading date
mean_days <- gold %>%
  mutate(Year = year(Date)) %>%
  group_by(Year) %>%
  summarise(count = n()) %>%
  pull(count) %>%
  mean()

mean_days = floor(mean_days)
  

#basic information
n = nrow(gold)
y_start = 2014
m_start = 1
# trading days per year
ts = ts(gold$boxcox, start = c(y_start, m_start), frequency = mean_days)

#classical decomposition
gold_decomp <- decompose(ts, type = "additive")
plot(gold_decomp$trend, main = "main trend", ylab = "price after boxcox transformation")

```
```{r}
# seasonality analysis per year
plot(gold_decomp$seasonal, main = "seasonal trend", ylab = "price after boxcox transformation")
```
```{r}
# stationary error
plot(gold_decomp$random, main = "stationary error", ylab = "price after boxcox transformation")
```
```{r}
#adf test after classical decomposition
# check if the residuals are stationary
# H0: the residuals are not pass as stationary time series

adf.test(na.omit(gold_decomp$random))
```
The p-value < alpha = 0.05, so we can reject the H0, which means the residuals pass as stationary time series.


```{r}
# use sample ACF to check residual
residuals <- na.omit(gold_decomp$random)

acf(residuals, main="ACF of Residuals", lag.max = 20)

pacf(residuals, main="PACF of Residuals")
```
The sample ACF plot shows that there are many lags exceed the blue dotted line, which means the residuals of the model have significant autocorrelation.

```{r}
# check residuals using another test (Ljung-Box Test)
# H0: the residuals are white noise (no autocorrelation)
Box.test(residuals, type = "Ljung-Box")
```
The p-value < alpha = 0.05, which means we have strong evidence to reject the H0, the residuals have autocorrelation and the model does not adequately capture the trend of the data. So we are going to fit a ARMA model.

```{r}
#arma model 
model1 = auto.arima(residuals, ic = "aic", stationary = TRUE)
summary(model1)
```
```{r}
acf(residuals(model1), main = "ACF of New Residuals") 
```
```{r}
pacf(residuals(model1), main = "PACF of New Residuals", lag.max = 50) 
```  

```{r}
hist(residuals(model1))
```
```{r}
# check residuals using another test (Ljung-Box Test)
# H0: the residuals are white noise (no autocorrelation)
Box.test(residuals(model1), type = "Ljung-Box")
```
The p-value > alpha = 0.05, which means we can not reject the H0. The residuals have no autocorrelation and the model can capture the trend of the data.

```{r}
#main trend
#model samples
trend = na.omit(gold_decomp$trend)
df_trend = data.frame(t = seq_along(trend), trend = trend)

#trend fitting
trend_model = lm(trend ~ t, data = df_trend)

#model summary
summary(trend_model)
```
```{r}
#sesasonal trend
#model samples
seasonal = head(gold_decomp$seasonal, mean_days)
df_seasonal = data.frame(t = seq_along(seasonal), seasonal = seasonal)
```

```{r}
#linear model
seasonal1 = lm(seasonal ~ t, data = df_seasonal)
summary(seasonal1)
```
linear model can only interpret 9% information in seasonal trend. Try higher degree polynomial.

```{r}
#polynomial model
degrees = 2:5
results = data.frame(degree = degrees, adj_r2 = NA)

#degree 2
model2 = lm(seasonal ~ poly(t, 2), data = df_seasonal)

#adj r square
fit_summary = summary(model2)
adj_r2 = fit_summary$adj.r.squared
results$adj_r2[results$degree == 2] = adj_r2
print(summary(model2))

#degree 3
model3 = lm(seasonal ~ poly(t, 3), data = df_seasonal)

#adj r square
fit_summary = summary(model3)
adj_r2 = fit_summary$adj.r.squared
results$adj_r2[results$degree == 3] = adj_r2
print(summary(model3))

#degree 4
model4 = lm(seasonal ~ poly(t, 4), data = df_seasonal)

#adj r square
fit_summary = summary(model4)
adj_r2 = fit_summary$adj.r.squared
results$adj_r2[results$degree == 4] = adj_r2
print(summary(model4))

#degree 5
model5 = lm(seasonal ~ poly(t, 5), data = df_seasonal)

#adj r square
fit_summary = summary(model5)
adj_r2 = fit_summary$adj.r.squared
results$adj_r2[results$degree == 5] = adj_r2
print(summary(model5))


```

```{r}
par(mfrow = c(1, 2))
plot(results$degree, results$adj_r2, type = "l", ylim = c(0,1), xlab = "degree", ylab = "adjusted r^2", main = "adjusted r^2")
plot(results$degree, results$adj_r2, type = "l", xlab = "degree", ylab = "adjusted r^2", main = "adjusted r^2 (zoom in)", ylim = c(0.64, 0.67))
```
Adjustment r square value going flat after degree 4. The values of adj_r2 are very close even though there shows a big increase from degree 3 to 4. Try residual analysis.

```{r}
#residuals from polynomial models
residuals_poly = data_frame(index = seq(length(residuals(model2))), r2 = residuals(model2), r3 = residuals(model3), r4 = residuals(model4), r5 = residuals(model5))

#residual plot
par(mfrow = c(2, 2))
plot(x = residuals_poly$index, y = residuals_poly$r2, main = "residual plot (degree 2)", xlab = "index", ylab = "residual")
plot(x = residuals_poly$index, y = residuals_poly$r3, main = "residual plot (degree 3)", xlab = "index", ylab = "residual")
plot(x = residuals_poly$index, y = residuals_poly$r4, main = "residual plot (degree 4)", xlab = "index", ylab = "residual")
plot(x = residuals_poly$index, y = residuals_poly$r5, main = "residual plot (degree 5)", xlab = "index", ylab = "residual")

par(mfrow = c(2, 2))
hist(x = residuals_poly$r2, main = "residual histogram (degree 2)", xlab = "residuals")
hist(x = residuals_poly$r3, main = "residual histogram (degree 3)", xlab = "residuals")
hist(x = residuals_poly$r4, main = "residual histogram (degree 4)", xlab = "residuals")
hist(x = residuals_poly$r5, main = "residual histogram (degree 5)", xlab = "residuals")
```
polynomial perform badly. Try fourier algorithm

```{r}
#fourier algorithm
P = 255
K = 2:9
r2_fr = data.frame(K = K, adj_r2 = NA)

fr_model = list()

#iterate from K = 2 to 9, save 
for (K in K) {
  #generate fourier term
  for (k in 1:K) {
    sin_col = paste0("sin", k)
    cos_col = paste0("cos", k)
    
    df_seasonal[[sin_col]] <- sin(2*pi * k * df_seasonal$t / P)
    df_seasonal[[cos_col]] <- cos(2*pi * k * df_seasonal$t / P)
  }
  
  vars = c()
  for (k in 1:K) {
    vars = c(vars, paste0("sin", k), paste0("cos", k))
  }
  
  fmla = as.formula(
    paste("seasonal ~", paste(vars, collapse = " + "))
  )
  
  #fit formula
  fit_fourier = lm(fmla, data = df_seasonal)
  r2 = summary(fit_fourier)$adj.r.squared
  r2_fr$adj_r2[r2_fr$K == K] = r2
  
  fr_model[[paste0("K_", K)]] = fit_fourier
}
```
```{r}
#plot adj r2
par(mfrow = c(1, 2))
plot(r2_fr$K, r2_fr$adj_r2, type = "l", ylim = c(0, 1), main = "adjusted r^2", ylab = "adjusted r^2", xlab ="K")
plot(r2_fr$K, r2_fr$adj_r2, type = "l", main = "adjusted r^2 (zoom in)", ylab = "adjusted r^2", xlab ="K")
```

```{r}
#rough component
random = gold_decomp$random
random = na.omit(random)
```

```{r}
#125 days prediction, from 11/6/2024 (data end date) to 3/11/2025 (date doing prediction)
h = 125

#main trend prediction
n = nrow(df_trend) #number of observation we have

predict = (n + 1) : (n + h) #index for prediction

trend_pred = predict(trend_model, newdata = data.frame(t = predict))
trend_pred = trend_pred
```

```{r}
#main trend prediction plot
plot(x = 1:125, y = InvBoxCox(trend_pred, lambda), type = "l", main = "main trend 125 days prediction", ylab = "price", xlab = "days after data ends")
```

```{r}
#seasonal trend prediction
n = nrow(df_seasonal) #number of observation we have

K = 7

fr = data.frame(t = predict)

#generate fourier term
for (k in 1:K) {
  sin_col = paste0("sin", k)
  cos_col = paste0("cos", k)
  
  fr[[sin_col]] <- sin(2*pi * k * fr$t / P)
  fr[[cos_col]] <- cos(2*pi * k * fr$t / P)
}

seasonal_pred = predict(fr_model[[6]], newdata = fr)
seasonal_pred = seasonal_pred
```

```{r}
#plot seasonal prediction
plot(x = 1:125, y = InvBoxCox(seasonal_pred, lambda), type = "l", main = "seasonal trend 125 days prediction", ylab = "price", xlab = "days after data ends")
```
```{r}
#smooth component prediction
smooth = trend_pred + seasonal_pred
plot(x = 1:125, y = InvBoxCox(smooth, lambda = lambda), type = "l", main = "smooth component 125 days prediction", ylab = "price", xlab = "days after data ends")
```
```{r}
#rough component prediction
rough_pred = forecast(model1, h = 125)
```

```{r}
#plot rough 
point_forecast = InvBoxCox(as.numeric(rough_pred$mean), lambda = lambda)     
lower_80 = InvBoxCox(rough_pred$lower[,1], lambda = lambda) 
upper_80 = InvBoxCox(rough_pred$upper[,1], lambda = lambda)           
lower_95 = InvBoxCox(rough_pred$lower[,2], lambda = lambda)             
upper_95 = InvBoxCox(rough_pred$upper[,2], lambda = lambda)    

plot(1:125, point_forecast, type="l",
     ylim = range(lower_95, upper_95),
     xlab = "days after data ends",
     ylab = "Forecast",
     main = "rough component 125 days prediction", 
     )

#confidence interval
lines(1:125, lower_80, col="blue", lty=2)
lines(1:125, upper_80, col="blue", lty=2)
lines(1:125, lower_95, col="red",  lty=2)
lines(1:125, upper_95, col="red",  lty=2)

legend("bottomleft",
       c("Mean Forecast", "95% CI", "80% CI"),
       col = c("black", "red", "blue"),
       lty = c(1, 2, 2),
       lwd = c(2, 1, 1),
       bty="n", 
       border=NA,
       cex=0.75)


```

```{r}
#final prediction
final_pred <- InvBoxCox(trend_pred + seasonal_pred + point_forecast, lambda)
plot(final_pred, type="l", main = "final 125 days prediction", ylab = "price", xlab = "days after data ends")
```
```{r}
tail(final_pred)
```


