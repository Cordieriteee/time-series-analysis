```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev="httpgd", fig.show="hold")
```

```{r}
library(dplyr)
library(tseries)
library(forecast)
library(lubridate)
```

```{r}
gold <- read.csv("Gold Price.csv")
```

```{r}
gold$Date <- as.Date(gold$Date)
gold <- gold[order(gold$Date),]
```

```{r}
head(gold)
```

Data preparation
```{r}
#gold price statistics
summary(gold$Price)
```

```{r}
# check if there is missing value
sum(is.na(gold))

# detect outliers using IQR
boxplot(gold$Price, main="Gold Price Boxplot", col="lightblue")

Q1 <- quantile(gold$Price, 0.25)
Q3 <- quantile(gold$Price, 0.75)
IQR_value <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

outliers <- gold$Price[gold$Price < lower_bound | gold$Price > upper_bound]
print(outliers)

```

```{r}
plot(gold$Date, gold$Price)
```
From the plot of the gold price, the plot seems not stationary because the trend of the price is increasing.

Check stationary using Augmented Dickey-Fuller (ADF test):
```{r}
adf.test(gold$Price)
```
By the Augmented Dickey-Fuller Test, the p-value is greater then alpha=0.05, which means we don't have enough evidence to reject that the data is not stationary. 


```{r}
# check symmetry
hist(gold$Price, main = "Histogram of the Gold Price", xlab = "Gold Price")
```
The histogram is right skewed.Try log transformation.

```{r}
#log transformation
gold$logPrice <- log(gold$Price)

#check symmetry
hist(gold$logPrice, main = "Histogram of Log Gold Price", xlab = "Log of Gold Price")
```
```{r}
#adf test after log transformation
adf.test(gold$logPrice)
```
Log transformation does not perform well. Try BoxCox transformation.

```{r}
#BoxCox transformation
lambda = BoxCox.lambda(gold$Price)
print(paste("best lambda is:" ,lambda))
```

```{r}
gold$boxcox = BoxCox(gold$Price, lambda = lambda)
hist(gold$boxcox, main = "histogram of BoxCox transformation")
```

```{r}
#adf test after boxcox transformation
adf.test(gold$boxcox)
```
BoxCox transformation dose not works well. However, p-value decreases from 0.9002 to 0.3328 and the right skewed trend is weakened. We choose to retain BoxCox transformation.

```{r}
#mean trading date
mean_days <- gold %>%
  mutate(Year = year(Date)) %>%
  group_by(Year) %>%
  summarise(count = n()) %>%
  pull(count) %>%
  mean()

mean_days = floor(mean_days)
  

#basic information
n = nrow(gold)
y_start = 2014
m_start = 1
# trading days per year
ts = ts(gold$boxcox, start = c(y_start, m_start), frequency = mean_days)

#classical decomposition
gold_decomp <- decompose(ts, type = "additive")
plot(gold_decomp$trend)

```
```{r}
# seasonality analysis per year
plot(gold_decomp$seasonal)
```
```{r}
# stationary error
plot(gold_decomp$random)
```
```{r}
#adf test after classical decomposition
# check if the residuals are stationary
# H0: the residuals are not pass as stationary time series

adf.test(na.omit(gold_decomp$random))
```
The p-value < alpha = 0.05, so we can reject the H0, which means the residuals pass as stationary time series.


```{r}
# use sample ACF to check residual
residuals <- na.omit(gold_decomp$random)

acf(residuals, main="ACF of Residuals", lag.max = 20)

pacf(residuals, main="PACF of Residuals")
```
The sample ACF plot shows that there are many lags exceed the blue dotted line, which means the residuals of the model have significant autocorrelation.

```{r}
# check residuals using another test (Ljung-Box Test)
# H0: the residuals are white noise (no autocorrelation)
Box.test(residuals, type = "Ljung-Box")
```
The p-value < alpha = 0.05, which means we have strong evidence to reject the H0, the residuals have autocorrelation and the model does not adequately capture the trend of the data. So we are going to fit a ARMA model.

```{r}
#arma model 
model1 = auto.arima(residuals, ic = "aic", stationary = TRUE)
summary(model1)
```
```{r}
acf(residuals(model1), main = "ACF of New Residuals") 
```
```{r}
pacf(residuals(model1), main = "PACF of New Residuals", lag.max = 50) 
```  

```{r}
hist(residuals(model1))
```
```{r}
# check residuals using another test (Ljung-Box Test)
# H0: the residuals are white noise (no autocorrelation)
Box.test(residuals(model1), type = "Ljung-Box")
```
The p-value > alpha = 0.05, which means we can not reject the H0. The residuals have no autocorrelation and the model can capture the trend of the data.

```{r}
#main trend
#model samples
trend = na.omit(gold_decomp$trend)
df_trend = data.frame(t = seq_along(trend), trend = trend)

#trend fitting
trend_model = lm(trend ~ t, data = df_trend)

#model summary
summary(trend_model)
```
```{r}
#sesasonal trend
#model samples
seasonal = head(gold_decomp$seasonal, mean_days)
df_seasonal = data.frame(t = seq_along(seasonal), seasonal = seasonal)
```
```{r}
#linear model
seasonal1 = lm(seasonal ~ t, data = df_seasonal)
summary(seasonal1)
```
linear model can only interpret 9% information in seasonal trend. Try higher degree polynomial.

```{r}
#polynomial model
degrees = 2:5
results = data.frame(degree = degrees, adj_r2 = NA)

#degree 2
model2 = lm(seasonal ~ poly(t, 2), data = df_seasonal)

#adj r square
fit_summary = summary(model2)
adj_r2 = fit_summary$r.squared
results$adj_r2[results$degree == 2] = adj_r2
print(summary(model2))

#degree 3
model3 = lm(seasonal ~ poly(t, 3), data = df_seasonal)

#adj r square
fit_summary = summary(model3)
adj_r2 = fit_summary$r.squared
results$adj_r2[results$degree == 3] = adj_r2
print(summary(model3))

#degree 4
model4 = lm(seasonal ~ poly(t, 4), data = df_seasonal)

#adj r square
fit_summary = summary(model4)
adj_r2 = fit_summary$r.squared
results$adj_r2[results$degree == 4] = adj_r2
print(summary(model4))

#degree 5
model5 = lm(seasonal ~ poly(t, 5), data = df_seasonal)

#adj r square
fit_summary = summary(model5)
adj_r2 = fit_summary$r.squared
results$adj_r2[results$degree == 5] = adj_r2
print(summary(model5))
```

```{r}
par(mfrow = c(1, 2))
plot(results$degree, results$adj_r2, type = "l", ylim = c(0,1))
plot(results$degree, results$adj_r2, type = "l", ylim = c(0.65, 0.67))
```
Adjustment r square value going flat after degree 4. The values of adj_r2 are very close even though there shows a big increase from degree 3 to 4. Try residual analysis.

```{r}
#residuals from polynomial models
residuals_poly = data_frame(index = seq(length(residuals(model2))), r2 = residuals(model2), r3 = residuals(model3), r4 = residuals(model4), r5 = residuals(model5))

#residual plot
par(mfrow = c(2, 2))
plot(x = residuals_poly$index, y = residuals_poly$r2)
plot(x = residuals_poly$index, y = residuals_poly$r3)
plot(x = residuals_poly$index, y = residuals_poly$r4)
plot(x = residuals_poly$index, y = residuals_poly$r5)

par(mfrow = c(2, 2))
hist(x = residuals_poly$r2)
hist(x = residuals_poly$r3)
hist(x = residuals_poly$r4)
hist(x = residuals_poly$r5)
```









